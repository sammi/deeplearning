{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['Dataset', 'SimpleDataset', 'ArrayDataset', 'RecordFileDataset']\n",
    "\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, idx):\n",
    "        raise NotImplementedError\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "    def transform(self, fn, lazy=True):\n",
    "        trans = _LazyTransformDataset(self, fn)\n",
    "        if lazy:\n",
    "            return trans\n",
    "        return SimpleDataset([i for i in trans])\n",
    "    \n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[idx]\n",
    "    \n",
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, *args):\n",
    "        assert len(args) > 0, \"Needs at least 1 arrays\"\n",
    "        self._length = len(args[0])\n",
    "        self._data = []\n",
    "        for i, data in enumerate(args):\n",
    "            assert len(data) == self._length, \\\n",
    "                \"All arrays must have the same length; array[0] has length %d \" \\\n",
    "                \"while array[%d] has %d.\" % (self._length, i+1, len(data))\n",
    "            if isinstance(data, ndarray.NDArray) and len(data.shape) == 1:\n",
    "                data = data.asnumpy()\n",
    "            self._data.append(data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self._data) == 1:\n",
    "            return self._data[0][idx]\n",
    "        else:\n",
    "            return tuple(data[idx] for data in self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "    \n",
    "class RecordFileDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.idx_file = os.path.splitext(filename)[0] + '.idx'\n",
    "        self.filename = filename\n",
    "        self._record = recordio.MXIndexedRecordIO(self.idx_file, self.filename, 'r')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._record.read_idx(self._record.keys[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._record.keys)\n",
    "\n",
    "class _DownloadedDataset(Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        super(_DownloadedDataset, self).__init__()\n",
    "        self._transform = transform\n",
    "        self._data = None\n",
    "        self._label = None\n",
    "        root = os.path.expanduser(root)\n",
    "        self._root = root\n",
    "        if not os.path.isdir(root):\n",
    "            os.makedirs(root)\n",
    "        self._get_data()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self._transform is not None:\n",
    "            return self._transform(self._data[idx], self._label[idx])\n",
    "        return self._data[idx], self._label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._label)\n",
    "\n",
    "    def _get_data(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class _LazyTransformDataset(Dataset):\n",
    "    def __init__(self, data, fn):\n",
    "        self._data = data\n",
    "        self._fn = fn\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self._data[idx]\n",
    "        if isinstance(item, tuple):\n",
    "            return self._fn(*item)\n",
    "        return self._fn(item)\n",
    "    def transform_first(self, fn, lazy=True):\n",
    "        def base_fn(x, *args):\n",
    "            if args:\n",
    "                return (fn(x),) + args\n",
    "            return fn(x)\n",
    "        return self.transform(base_fn, lazy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "import sys\n",
    "import multiprocessing\n",
    "import multiprocessing.queues\n",
    "from multiprocessing.reduction import ForkingPickler\n",
    "import threading\n",
    "import numpy as np\n",
    "\n",
    "class ConnectionWrapper(object):\n",
    "    def __init__(self, conn):\n",
    "        self._conn = conn\n",
    "    def send(self, obj):\n",
    "        buf = io.BytesIO()\n",
    "        ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(obj)\n",
    "        self.send_bytes(buf.getvalue())\n",
    "    def recv(self):\n",
    "        buf = self.recv_bytes()\n",
    "        return pickle.loads(buf)\n",
    "    def __getattr__(self, name):\n",
    "        attr = self.__dict__.get('_conn', None)\n",
    "        return getattr(attr, name)\n",
    "    \n",
    "class Queue(multiprocessing.queues.Queue):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if sys.version_info[0] <= 2:\n",
    "            super(Queue, self).__init__(*args, **kwargs)\n",
    "        else:\n",
    "            super(Queue, self).__init__(*args, ctx=multiprocessing.get_context(),\n",
    "                                        **kwargs)\n",
    "        self._reader = ConnectionWrapper(self._reader)\n",
    "        self._writer = ConnectionWrapper(self._writer)\n",
    "        self._send = self._writer.send\n",
    "        self._recv = self._reader.recv\n",
    "        \n",
    "class SimpleQueue(multiprocessing.queues.SimpleQueue):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if sys.version_info[0] <= 2:\n",
    "            super(SimpleQueue, self).__init__(*args, **kwargs)\n",
    "        else:\n",
    "            super(SimpleQueue, self).__init__(*args, ctx=multiprocessing.get_context(),\n",
    "                                              **kwargs)\n",
    "        self._reader = ConnectionWrapper(self._reader)\n",
    "        self._writer = ConnectionWrapper(self._writer)\n",
    "        self._send = self._writer.send\n",
    "        self._recv = self._reader.recv\n",
    "        \n",
    "def default_batchify_fn(data):\n",
    "    if isinstance(data[0], nd.NDArray):\n",
    "        return nd.stack(*data)\n",
    "    elif isinstance(data[0], tuple):\n",
    "        data = zip(*data)\n",
    "        return [default_batchify_fn(i) for i in data]\n",
    "    else:\n",
    "        data = np.asarray(data)\n",
    "        return nd.array(data, dtype=data.dtype)\n",
    "\n",
    "\n",
    "def default_mp_batchify_fn(data):\n",
    "    if isinstance(data[0], nd.NDArray):\n",
    "        out = nd.empty((len(data),) + data[0].shape, dtype=data[0].dtype,\n",
    "                       ctx=context.Context('cpu_shared', 0))\n",
    "        return nd.stack(*data, out=out)\n",
    "    elif isinstance(data[0], tuple):\n",
    "        data = zip(*data)\n",
    "        return [default_mp_batchify_fn(i) for i in data]\n",
    "    else:\n",
    "        data = np.asarray(data)\n",
    "        return nd.array(data, dtype=data.dtype,\n",
    "                        ctx=context.Context('cpu_shared', 0))\n",
    "\n",
    "\n",
    "def _as_in_context(data, ctx):\n",
    "    if isinstance(data, nd.NDArray):\n",
    "        return data.as_in_context(ctx)\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return [_as_in_context(d, ctx) for d in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "def worker_loop_v1(dataset, key_queue, data_queue, batchify_fn):\n",
    "    while True:\n",
    "        idx, samples = key_queue.get()\n",
    "        if idx is None:\n",
    "            break\n",
    "        batch = batchify_fn([dataset[i] for i in samples])\n",
    "        data_queue.put((idx, batch))\n",
    "\n",
    "def fetcher_loop_v1(data_queue, data_buffer, pin_memory=False, data_buffer_lock=None):\n",
    "    while True:\n",
    "        idx, batch = data_queue.get()\n",
    "        if idx is None:\n",
    "            break\n",
    "        if pin_memory:\n",
    "            batch = _as_in_context(batch, context.cpu_pinned())\n",
    "        else:\n",
    "            batch = _as_in_context(batch, context.cpu())\n",
    "        if data_buffer_lock is not None:\n",
    "            with data_buffer_lock:\n",
    "                data_buffer[idx] = batch\n",
    "        else:\n",
    "            data_buffer[idx] = batch\n",
    "\n",
    "class _MultiWorkerIterV1(object):\n",
    "    def __init__(self, num_workers, dataset, batchify_fn, batch_sampler, pin_memory=False,\n",
    "                 worker_fn=worker_loop_v1):\n",
    "        assert num_workers > 0, \"_MultiWorkerIter is not for {} workers\".format(num_workers)\n",
    "        self._num_workers = num_workers\n",
    "        self._dataset = dataset\n",
    "        self._batchify_fn = batchify_fn\n",
    "        self._batch_sampler = batch_sampler\n",
    "        self._key_queue = Queue()\n",
    "        self._data_queue = Queue() if sys.version_info[0] <= 2 else SimpleQueue()\n",
    "\n",
    "        self._data_buffer = {}\n",
    "        self._data_buffer_lock = threading.Lock()\n",
    "\n",
    "        self._rcvd_idx = 0\n",
    "        self._sent_idx = 0\n",
    "        self._iter = iter(self._batch_sampler)\n",
    "        self._shutdown = False\n",
    "\n",
    "        workers = []\n",
    "        for _ in range(self._num_workers):\n",
    "            worker = multiprocessing.Process(\n",
    "                target=worker_fn,\n",
    "                args=(self._dataset, self._key_queue, self._data_queue, self._batchify_fn))\n",
    "            worker.daemon = True\n",
    "            worker.start()\n",
    "            workers.append(worker)\n",
    "        self._workers = workers\n",
    "\n",
    "        self._fetcher = threading.Thread(\n",
    "            target=fetcher_loop_v1,\n",
    "            args=(self._data_queue, self._data_buffer, pin_memory, self._data_buffer_lock))\n",
    "        self._fetcher.daemon = True\n",
    "        self._fetcher.start()\n",
    "\n",
    "        # pre-fetch\n",
    "        for _ in range(2 * self._num_workers):\n",
    "            self._push_next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batch_sampler)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.shutdown()\n",
    "\n",
    "    def _push_next(self):\n",
    "        r = next(self._iter, None)\n",
    "        if r is None:\n",
    "            return\n",
    "        self._key_queue.put((self._sent_idx, r))\n",
    "        self._sent_idx += 1\n",
    "\n",
    "    def __next__(self):\n",
    "        assert not self._shutdown, \"call __next__ after shutdown is forbidden\"\n",
    "        if self._rcvd_idx == self._sent_idx:\n",
    "            assert not self._data_buffer, \"Data buffer should be empty at this moment\"\n",
    "            self.shutdown()\n",
    "            raise StopIteration\n",
    "\n",
    "        while True:\n",
    "            if self._rcvd_idx in self._data_buffer:\n",
    "                with self._data_buffer_lock:\n",
    "                    batch = self._data_buffer.pop(self._rcvd_idx)\n",
    "                self._rcvd_idx += 1\n",
    "                self._push_next()\n",
    "                return batch\n",
    "\n",
    "    def next(self):\n",
    "        return self.__next__()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def shutdown(self):\n",
    "        if not self._shutdown:\n",
    "            self._data_queue.put((None, None))\n",
    "            self._fetcher.join()\n",
    "            for _ in range(self._num_workers):\n",
    "                self._key_queue.put((None, None))\n",
    "            for w in self._workers:\n",
    "                if w.is_alive():\n",
    "                    w.terminate()\n",
    "            self._shutdown = True\n",
    "            \n",
    "            \n",
    "class DataLoaderV1(object):\n",
    "    def __init__(self, dataset, batch_size=None, shuffle=False, sampler=None,\n",
    "                 last_batch=None, batch_sampler=None, batchify_fn=None,\n",
    "                 num_workers=0, pin_memory=False):\n",
    "        self._dataset = dataset\n",
    "        self._pin_memory = pin_memory\n",
    "\n",
    "        if batch_sampler is None:\n",
    "            if batch_size is None:\n",
    "                raise ValueError(\"batch_size must be specified unless \" \\\n",
    "                                 \"batch_sampler is specified\")\n",
    "            if sampler is None:\n",
    "                if shuffle:\n",
    "                    sampler = _sampler.RandomSampler(len(dataset))\n",
    "                else:\n",
    "                    sampler = _sampler.SequentialSampler(len(dataset))\n",
    "            elif shuffle:\n",
    "                raise ValueError(\"shuffle must not be specified if sampler is specified\")\n",
    "\n",
    "            batch_sampler = _sampler.BatchSampler(\n",
    "                sampler, batch_size, last_batch if last_batch else 'keep')\n",
    "        elif batch_size is not None or shuffle or sampler is not None or \\\n",
    "                last_batch is not None:\n",
    "            raise ValueError(\"batch_size, shuffle, sampler and last_batch must \" \\\n",
    "                             \"not be specified if batch_sampler is specified.\")\n",
    "\n",
    "        self._batch_sampler = batch_sampler\n",
    "        self._num_workers = num_workers if num_workers >= 0 else 0\n",
    "        if batchify_fn is None:\n",
    "            if num_workers > 0:\n",
    "                self._batchify_fn = default_mp_batchify_fn\n",
    "            else:\n",
    "                self._batchify_fn = default_batchify_fn\n",
    "        else:\n",
    "            self._batchify_fn = batchify_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self._num_workers == 0:\n",
    "            def same_process_iter():\n",
    "                for batch in self._batch_sampler:\n",
    "                    ret = self._batchify_fn([self._dataset[idx] for idx in batch])\n",
    "                    if self._pin_memory:\n",
    "                        ret = _as_in_context(ret, context.cpu_pinned())\n",
    "                    yield ret\n",
    "            return same_process_iter()\n",
    "\n",
    "        return _MultiWorkerIterV1(self._num_workers, self._dataset,\n",
    "                                  self._batchify_fn, self._batch_sampler, self._pin_memory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batch_sampler)\n",
    "\n",
    "_worker_dataset = None\n",
    "\n",
    "def _worker_initializer(dataset):\n",
    "    global _worker_dataset\n",
    "    _worker_dataset = dataset\n",
    "\n",
    "def _worker_fn(samples, batchify_fn):\n",
    "    global _worker_dataset\n",
    "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
    "    buf = io.BytesIO()\n",
    "    ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(batch)\n",
    "    return buf.getvalue()\n",
    "\n",
    "class _MultiWorkerIter(object):\n",
    "    def __init__(self, worker_pool, batchify_fn, batch_sampler, pin_memory=False,\n",
    "                 worker_fn=_worker_fn, prefetch=0):\n",
    "        self._worker_pool = worker_pool\n",
    "        self._batchify_fn = batchify_fn\n",
    "        self._batch_sampler = batch_sampler\n",
    "        self._data_buffer = {}\n",
    "        self._rcvd_idx = 0\n",
    "        self._sent_idx = 0\n",
    "        self._iter = iter(self._batch_sampler)\n",
    "        self._worker_fn = worker_fn\n",
    "        self._pin_memory = pin_memory\n",
    "        # pre-fetch\n",
    "        for _ in range(prefetch):\n",
    "            self._push_next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batch_sampler)\n",
    "\n",
    "    def _push_next(self):\n",
    "        r = next(self._iter, None)\n",
    "        if r is None:\n",
    "            return\n",
    "        async_ret = self._worker_pool.apply_async(self._worker_fn, (r, self._batchify_fn))\n",
    "        self._data_buffer[self._sent_idx] = async_ret\n",
    "        self._sent_idx += 1\n",
    "\n",
    "    def __next__(self):\n",
    "        self._push_next()\n",
    "        if self._rcvd_idx == self._sent_idx:\n",
    "            assert not self._data_buffer, \"Data buffer should be empty at this moment\"\n",
    "            raise StopIteration\n",
    "\n",
    "        assert self._rcvd_idx < self._sent_idx, \"rcvd_idx must be smaller than sent_idx\"\n",
    "        assert self._rcvd_idx in self._data_buffer, \"fatal error with _push_next, rcvd_idx missing\"\n",
    "        ret = self._data_buffer.pop(self._rcvd_idx)\n",
    "        batch = pickle.loads(ret.get())\n",
    "        if self._pin_memory:\n",
    "            batch = _as_in_context(batch, context.cpu_pinned())\n",
    "        batch = batch[0] if len(batch) == 1 else batch\n",
    "        self._rcvd_idx += 1\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        return self.__next__()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=None, shuffle=False, sampler=None,\n",
    "                 last_batch=None, batch_sampler=None, batchify_fn=None,\n",
    "                 num_workers=0, pin_memory=False, prefetch=None):\n",
    "        self._dataset = dataset\n",
    "        self._pin_memory = pin_memory\n",
    "\n",
    "        if batch_sampler is None:\n",
    "            if batch_size is None:\n",
    "                raise ValueError(\"batch_size must be specified unless \" \\\n",
    "                                 \"batch_sampler is specified\")\n",
    "            if sampler is None:\n",
    "                if shuffle:\n",
    "                    sampler = _sampler.RandomSampler(len(dataset))\n",
    "                else:\n",
    "                    sampler = _sampler.SequentialSampler(len(dataset))\n",
    "            elif shuffle:\n",
    "                raise ValueError(\"shuffle must not be specified if sampler is specified\")\n",
    "\n",
    "            batch_sampler = _sampler.BatchSampler(\n",
    "                sampler, batch_size, last_batch if last_batch else 'keep')\n",
    "        elif batch_size is not None or shuffle or sampler is not None or \\\n",
    "                last_batch is not None:\n",
    "            raise ValueError(\"batch_size, shuffle, sampler and last_batch must \" \\\n",
    "                             \"not be specified if batch_sampler is specified.\")\n",
    "\n",
    "        self._batch_sampler = batch_sampler\n",
    "        self._num_workers = num_workers if num_workers >= 0 else 0\n",
    "        self._worker_pool = None\n",
    "        self._prefetch = max(0, int(prefetch) if prefetch is not None else 2 * self._num_workers)\n",
    "        if self._num_workers > 0:\n",
    "            self._worker_pool = multiprocessing.Pool(\n",
    "                self._num_workers, initializer=_worker_initializer, initargs=[self._dataset])\n",
    "        if batchify_fn is None:\n",
    "            if num_workers > 0:\n",
    "                self._batchify_fn = default_mp_batchify_fn\n",
    "            else:\n",
    "                self._batchify_fn = default_batchify_fn\n",
    "        else:\n",
    "            self._batchify_fn = batchify_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self._num_workers == 0:\n",
    "            def same_process_iter():\n",
    "                for batch in self._batch_sampler:\n",
    "                    ret = self._batchify_fn([self._dataset[idx] for idx in batch])\n",
    "                    if self._pin_memory:\n",
    "                        ret = _as_in_context(ret, context.cpu_pinned())\n",
    "                    yield ret\n",
    "            return same_process_iter()\n",
    "\n",
    "        return _MultiWorkerIter(self._worker_pool, self._batchify_fn, self._batch_sampler,\n",
    "                                pin_memory=self._pin_memory, worker_fn=_worker_fn,\n",
    "                                prefetch=self._prefetch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batch_sampler)\n",
    "\n",
    "    def __del__(self):\n",
    "        if self._worker_pool:\n",
    "            assert isinstance(self._worker_pool, multiprocessing.pool.Pool)\n",
    "            self._worker_pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SequentialSampler(Sampler):\n",
    "    def __init__(self, length):\n",
    "        self._length = length\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self._length))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "\n",
    "class RandomSampler(Sampler):\n",
    "    def __init__(self, length):\n",
    "        self._length = length\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = np.arange(self._length)\n",
    "        np.random.shuffle(indices)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "\n",
    "class BatchSampler(Sampler):\n",
    "    def __init__(self, sampler, batch_size, last_batch='keep'):\n",
    "        self._sampler = sampler\n",
    "        self._batch_size = batch_size\n",
    "        self._last_batch = last_batch\n",
    "        self._prev = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch, self._prev = self._prev, []\n",
    "        for i in self._sampler:\n",
    "            batch.append(i)\n",
    "            if len(batch) == self._batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            if self._last_batch == 'keep':\n",
    "                yield batch\n",
    "            elif self._last_batch == 'discard':\n",
    "                return\n",
    "            elif self._last_batch == 'rollover':\n",
    "                self._prev = batch\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"last_batch must be one of 'keep', 'discard', or 'rollover', \" \\\n",
    "                    \"but got %s\"%self._last_batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._last_batch == 'keep':\n",
    "            return (len(self._sampler) + self._batch_size - 1) // self._batch_size\n",
    "        if self._last_batch == 'discard':\n",
    "            return len(self._sampler) // self._batch_size\n",
    "        if self._last_batch == 'rollover':\n",
    "            return (len(self._prev) + len(self._sampler)) // self._batch_size\n",
    "        raise ValueError(\n",
    "            \"last_batch must be one of 'keep', 'discard', or 'rollover', \" \\\n",
    "            \"but got %s\"%self._last_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.block import Block, HybridBlock\n",
    "from mxnet.gluon.nn import Sequential, HybridSequential\n",
    "\n",
    "class Compose(Sequential):\n",
    "    def __init__(self, transforms):\n",
    "        super(Compose, self).__init__()\n",
    "        transforms.append(None)\n",
    "        hybrid = []\n",
    "        for i in transforms:\n",
    "            if isinstance(i, HybridBlock):\n",
    "                hybrid.append(i)\n",
    "                continue\n",
    "            elif len(hybrid) == 1:\n",
    "                self.add(hybrid[0])\n",
    "                hybrid = []\n",
    "            elif len(hybrid) > 1:\n",
    "                hblock = HybridSequential()\n",
    "                for j in hybrid:\n",
    "                    hblock.add(j)\n",
    "                hblock.hybridize()\n",
    "                self.add(hblock)\n",
    "                hybrid = []\n",
    "\n",
    "            if i is not None:\n",
    "                self.add(i)\n",
    "\n",
    "\n",
    "class Cast(HybridBlock):\n",
    "    def __init__(self, dtype='float32'):\n",
    "        super(Cast, self).__init__()\n",
    "        self._dtype = dtype\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.cast(x, self._dtype)\n",
    "\n",
    "\n",
    "class ToTensor(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(ToTensor, self).__init__()\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.to_tensor(x)\n",
    "\n",
    "\n",
    "class Normalize(HybridBlock):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.normalize(x, self._mean, self._std)\n",
    "\n",
    "\n",
    "class RandomResizedCrop(Block):\n",
    "    def __init__(self, size, scale=(0.08, 1.0), ratio=(3.0/4.0, 4.0/3.0),\n",
    "                 interpolation=1):\n",
    "        super(RandomResizedCrop, self).__init__()\n",
    "        if isinstance(size, numeric_types):\n",
    "            size = (size, size)\n",
    "        self._args = (size, scale, ratio, interpolation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return image.random_size_crop(x, *self._args)[0]\n",
    "\n",
    "\n",
    "class CenterCrop(Block):\n",
    "    def __init__(self, size, interpolation=1):\n",
    "        super(CenterCrop, self).__init__()\n",
    "        if isinstance(size, numeric_types):\n",
    "            size = (size, size)\n",
    "        self._args = (size, interpolation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return image.center_crop(x, *self._args)[0]\n",
    "\n",
    "\n",
    "class Resize(Block):\n",
    "    def __init__(self, size, keep_ratio=False, interpolation=1):\n",
    "        super(Resize, self).__init__()\n",
    "        self._keep = keep_ratio\n",
    "        self._size = size\n",
    "        self._interpolation = interpolation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self._size, numeric_types):\n",
    "            if not self._keep:\n",
    "                wsize = self._size\n",
    "                hsize = self._size\n",
    "            else:\n",
    "                h, w, _ = x.shape\n",
    "                if h > w:\n",
    "                    wsize = self._size\n",
    "                    hsize = int(h * wsize / w)\n",
    "                else:\n",
    "                    hsize = self._size\n",
    "                    wsize = int(w * hsize / h)\n",
    "        else:\n",
    "            wsize, hsize = self._size\n",
    "        return image.imresize(x, wsize, hsize, self._interpolation)\n",
    "\n",
    "\n",
    "class RandomFlipLeftRight(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(RandomFlipLeftRight, self).__init__()\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_flip_left_right(x)\n",
    "\n",
    "\n",
    "class RandomFlipTopBottom(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(RandomFlipTopBottom, self).__init__()\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_flip_top_bottom(x)\n",
    "\n",
    "\n",
    "class RandomBrightness(HybridBlock):\n",
    "    def __init__(self, brightness):\n",
    "        super(RandomBrightness, self).__init__()\n",
    "        self._args = (max(0, 1-brightness), 1+brightness)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_brightness(x, *self._args)\n",
    "\n",
    "\n",
    "class RandomContrast(HybridBlock):\n",
    "    def __init__(self, contrast):\n",
    "        super(RandomContrast, self).__init__()\n",
    "        self._args = (max(0, 1-contrast), 1+contrast)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_contrast(x, *self._args)\n",
    "\n",
    "\n",
    "class RandomSaturation(HybridBlock):\n",
    "    def __init__(self, saturation):\n",
    "        super(RandomSaturation, self).__init__()\n",
    "        self._args = (max(0, 1-saturation), 1+saturation)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_saturation(x, *self._args)\n",
    "\n",
    "\n",
    "class RandomHue(HybridBlock):\n",
    "    def __init__(self, hue):\n",
    "        super(RandomHue, self).__init__()\n",
    "        self._args = (max(0, 1-hue), 1+hue)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_hue(x, *self._args)\n",
    "\n",
    "\n",
    "class RandomColorJitter(HybridBlock):\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        super(RandomColorJitter, self).__init__()\n",
    "        self._args = (brightness, contrast, saturation, hue)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_color_jitter(x, *self._args)\n",
    "\n",
    "\n",
    "class RandomLighting(HybridBlock):\n",
    "    def __init__(self, alpha):\n",
    "        super(RandomLighting, self).__init__()\n",
    "        self._alpha = alpha\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.image.random_lighting(x, self._alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-104dea9a8f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DownloadedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     def __init__(self, root=os.path.join(base.data_dir(), 'datasets', 'mnist'),\n\u001b[1;32m     10\u001b[0m                  train=True, transform=None):\n",
      "\u001b[0;32m<ipython-input-5-104dea9a8f6d>\u001b[0m in \u001b[0;36mMNIST\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DownloadedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     def __init__(self, root=os.path.join(base.data_dir(), 'datasets', 'mnist'),\n\u001b[0m\u001b[1;32m     10\u001b[0m                  train=True, transform=None):\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import tarfile\n",
    "import struct\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "class MNIST(_DownloadedDataset):\n",
    "    def __init__(self, root=os.path.join(base.data_dir(), 'datasets', 'mnist'),\n",
    "                 train=True, transform=None):\n",
    "        self._train = train\n",
    "        self._train_data = ('train-images-idx3-ubyte.gz', '6c95f4b05d2bf285e1bfb0e7960c31bd3b3f8a7d')\n",
    "        self._train_label = ('train-labels-idx1-ubyte.gz', '2a80914081dc54586dbdf242f9805a6b8d2a15fc')\n",
    "        self._test_data = ('t10k-images-idx3-ubyte.gz', 'c3a25af1f52dad7f726cce8cacb138654b760d48')\n",
    "        self._test_label = ('t10k-labels-idx1-ubyte.gz', '763e7fa3757d93b0cdec073cef058b2004252c17')\n",
    "        self._namespace = 'mnist'\n",
    "        super(MNIST, self).__init__(root, transform)\n",
    "\n",
    "    def _get_data(self):\n",
    "        if self._train:\n",
    "            data, label = self._train_data, self._train_label\n",
    "        else:\n",
    "            data, label = self._test_data, self._test_label\n",
    "\n",
    "        namespace = 'gluon/dataset/'+self._namespace\n",
    "        data_file = download(_get_repo_file_url(namespace, data[0]),\n",
    "                             path=self._root,\n",
    "                             sha1_hash=data[1])\n",
    "        label_file = download(_get_repo_file_url(namespace, label[0]),\n",
    "                              path=self._root,\n",
    "                              sha1_hash=label[1])\n",
    "\n",
    "        with gzip.open(label_file, 'rb') as fin:\n",
    "            struct.unpack(\">II\", fin.read(8))\n",
    "            label = np.frombuffer(fin.read(), dtype=np.uint8).astype(np.int32)\n",
    "\n",
    "        with gzip.open(data_file, 'rb') as fin:\n",
    "            struct.unpack(\">IIII\", fin.read(16))\n",
    "            data = np.frombuffer(fin.read(), dtype=np.uint8)\n",
    "            data = data.reshape(len(label), 28, 28, 1)\n",
    "\n",
    "        self._data = nd.array(data, dtype=data.dtype)\n",
    "        self._label = label\n",
    "\n",
    "\n",
    "class FashionMNIST(MNIST):\n",
    "    def __init__(self, root=os.path.join(base.data_dir(), 'datasets', 'fashion-mnist'), train=True, transform=None):\n",
    "        self._train = train\n",
    "        self._train_data = ('train-images-idx3-ubyte.gz',\n",
    "                            '0cf37b0d40ed5169c6b3aba31069a9770ac9043d')\n",
    "        self._train_label = ('train-labels-idx1-ubyte.gz',\n",
    "                             '236021d52f1e40852b06a4c3008d8de8aef1e40b')\n",
    "        self._test_data = ('t10k-images-idx3-ubyte.gz',\n",
    "                           '626ed6a7c06dd17c0eec72fa3be1740f146a2863')\n",
    "        self._test_label = ('t10k-labels-idx1-ubyte.gz',\n",
    "                            '17f9ab60e7257a1620f4ad76bbbaf857c3920701')\n",
    "        self._namespace = 'fashion-mnist'\n",
    "        super(MNIST, self).__init__(root, transform) # pylint: disable=bad-super-call\n",
    "\n",
    "\n",
    "class CIFAR10(dataset._DownloadedDataset):\n",
    "    def __init__(self, root=os.path.join(base.data_dir(), 'datasets', 'cifar10'), train=True, transform=None):\n",
    "        self._train = train\n",
    "        self._archive_file = ('cifar-10-binary.tar.gz', 'fab780a1e191a7eda0f345501ccd62d20f7ed891')\n",
    "        self._train_data = [('data_batch_1.bin', 'aadd24acce27caa71bf4b10992e9e7b2d74c2540'),\n",
    "                            ('data_batch_2.bin', 'c0ba65cce70568cd57b4e03e9ac8d2a5367c1795'),\n",
    "                            ('data_batch_3.bin', '1dd00a74ab1d17a6e7d73e185b69dbf31242f295'),\n",
    "                            ('data_batch_4.bin', 'aab85764eb3584312d3c7f65fd2fd016e36a258e'),\n",
    "                            ('data_batch_5.bin', '26e2849e66a845b7f1e4614ae70f4889ae604628')]\n",
    "        self._test_data = [('test_batch.bin', '67eb016db431130d61cd03c7ad570b013799c88c')]\n",
    "        self._namespace = 'cifar10'\n",
    "        super(CIFAR10, self).__init__(root, transform)\n",
    "\n",
    "    def _read_batch(self, filename):\n",
    "        with open(filename, 'rb') as fin:\n",
    "            data = np.frombuffer(fin.read(), dtype=np.uint8).reshape(-1, 3072+1)\n",
    "\n",
    "        return data[:, 1:].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1), \\\n",
    "               data[:, 0].astype(np.int32)\n",
    "\n",
    "    def _get_data(self):\n",
    "        if any(not os.path.exists(path) or not check_sha1(path, sha1)\n",
    "               for path, sha1 in ((os.path.join(self._root, name), sha1)\n",
    "                                  for name, sha1 in self._train_data + self._test_data)):\n",
    "            namespace = 'gluon/dataset/'+self._namespace\n",
    "            filename = download(_get_repo_file_url(namespace, self._archive_file[0]),\n",
    "                                path=self._root,\n",
    "                                sha1_hash=self._archive_file[1])\n",
    "\n",
    "            with tarfile.open(filename) as tar:\n",
    "                tar.extractall(self._root)\n",
    "\n",
    "        if self._train:\n",
    "            data_files = self._train_data\n",
    "        else:\n",
    "            data_files = self._test_data\n",
    "        data, label = zip(*(self._read_batch(os.path.join(self._root, name))\n",
    "                            for name, _ in data_files))\n",
    "        data = np.concatenate(data)\n",
    "        label = np.concatenate(label)\n",
    "\n",
    "        self._data = nd.array(data, dtype=data.dtype)\n",
    "        self._label = label\n",
    "\n",
    "\n",
    "class CIFAR100(CIFAR10):\n",
    "    def __init__(self, root=os.path.join(base.data_dir(), 'datasets', 'cifar100'),\n",
    "                 fine_label=False, train=True, transform=None):\n",
    "        self._train = train\n",
    "        self._archive_file = ('cifar-100-binary.tar.gz', 'a0bb982c76b83111308126cc779a992fa506b90b')\n",
    "        self._train_data = [('train.bin', 'e207cd2e05b73b1393c74c7f5e7bea451d63e08e')]\n",
    "        self._test_data = [('test.bin', '8fb6623e830365ff53cf14adec797474f5478006')]\n",
    "        self._fine_label = fine_label\n",
    "        self._namespace = 'cifar100'\n",
    "        super(CIFAR10, self).__init__(root, transform) # pylint: disable=bad-super-call\n",
    "\n",
    "    def _read_batch(self, filename):\n",
    "        with open(filename, 'rb') as fin:\n",
    "            data = np.frombuffer(fin.read(), dtype=np.uint8).reshape(-1, 3072+2)\n",
    "\n",
    "        return data[:, 2:].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1), \\\n",
    "               data[:, 0+self._fine_label].astype(np.int32)\n",
    "\n",
    "\n",
    "class ImageRecordDataset(dataset.RecordFileDataset):\n",
    "    def __init__(self, filename, flag=1, transform=None):\n",
    "        super(ImageRecordDataset, self).__init__(filename)\n",
    "        self._flag = flag\n",
    "        self._transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = super(ImageRecordDataset, self).__getitem__(idx)\n",
    "        header, img = recordio.unpack(record)\n",
    "        if self._transform is not None:\n",
    "            return self._transform(image.imdecode(img, self._flag), header.label)\n",
    "        return image.imdecode(img, self._flag), header.label\n",
    "\n",
    "\n",
    "class ImageFolderDataset(dataset.Dataset):\n",
    "    def __init__(self, root, flag=1, transform=None):\n",
    "        self._root = os.path.expanduser(root)\n",
    "        self._flag = flag\n",
    "        self._transform = transform\n",
    "        self._exts = ['.jpg', '.jpeg', '.png']\n",
    "        self._list_images(self._root)\n",
    "\n",
    "    def _list_images(self, root):\n",
    "        self.synsets = []\n",
    "        self.items = []\n",
    "\n",
    "        for folder in sorted(os.listdir(root)):\n",
    "            path = os.path.join(root, folder)\n",
    "            if not os.path.isdir(path):\n",
    "                warnings.warn('Ignoring %s, which is not a directory.'%path, stacklevel=3)\n",
    "                continue\n",
    "            label = len(self.synsets)\n",
    "            self.synsets.append(folder)\n",
    "            for filename in sorted(os.listdir(path)):\n",
    "                filename = os.path.join(path, filename)\n",
    "                ext = os.path.splitext(filename)[1]\n",
    "                if ext.lower() not in self._exts:\n",
    "                    warnings.warn('Ignoring %s of type %s. Only support %s'%(\n",
    "                        filename, ext, ', '.join(self._exts)))\n",
    "                    continue\n",
    "                self.items.append((filename, label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = image.imread(self.items[idx][0], self._flag)\n",
    "        label = self.items[idx][1]\n",
    "        if self._transform is not None:\n",
    "            return self._transform(img, label)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
